# -*- coding: utf-8 -*-
"""train_cnn_with_data_flow_from_directory.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QWsZWlUpcO_U2SxqHrOyjlzmOf_0DS95
"""

from google.colab import drive
drive.mount('/content/gdrive')

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import backend as K

# experiment = Experiment(api_key="4A4Q2iIKeHwXdfEDxbEhmkgoN",project_name="cnn")
# dimensions of our images.
# img_width, img_height = 150, 150
img_width, img_height = 224,224

train_data_dir = '/content/gdrive/My Drive/Dance_form_detection/Prepatred_data_Transfer_learning/train/'
validation_data_dir = '/content/gdrive/My Drive/Dance_form_detection/Prepatred_data_Transfer_learning/valid/'
test_data_dir  = '/content/gdrive/My Drive/Dance_form_detection/Prepatred_data_Transfer_learning/test/'
# nb_train_samples = 50
# nb_validation_samples = 10
epochs = 100
batch_size = 16

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)

# layer1
model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=input_shape))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
#layer2
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(3, 3)))
#layer3
model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
#layer4
model.add(Dense(8))
model.add(Activation('softmax'))

# Model compiling
# lr_schedule = optimizers.schedules.ExponentialDecay(
#     initial_learning_rate=1e-2,
#     decay_steps=10000,
#     decay_rate=0.9)
# optimizer = optimizers.SGD(learning_rate=0.01)
# optimizer = optimizers.Adam(lr=0.0001, decay=1e-6)
model.compile(loss='categorical_crossentropy', # or categorical_crossentropy
              optimizer='rmsprop',# or adagrad
              metrics=['accuracy'])

# this is the augmentation configuration we will use for training
train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    # width_shift_range=0.2,
    # height_shift_range=0.2,
    # rotation_range = 20,
    horizontal_flip=True)


# this is the augmentation configuration we will use for testing:
# only rescaling
valid_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    color_mode="rgb",
    shuffle=True,
    seed=42,
    class_mode='categorical')
print(train_generator.class_indices)

validation_generator = valid_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    color_mode="rgb",
    class_mode="categorical",
    shuffle=True,
    seed=42)
test_datagen=ImageDataGenerator(rescale=1./255.)
test_generator = test_datagen.flow_from_directory(
    directory=test_data_dir,
    target_size=(224, 224),
    color_mode="rgb",
    batch_size=1,
    class_mode=None,
    shuffle=False,
    seed=42)

# STEP_SIZE_TRAIN=nb_train_samples//train_generator.batch_size
# STEP_SIZE_VALID=nb_validation_samples//validation_generator.batch_size
STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size
STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size
model.fit_generator(
    train_generator,
    steps_per_epoch=STEP_SIZE_TRAIN,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=STEP_SIZE_VALID)

model.save('/content/gdrive/My Drive/Dance_form_detection/model/scratch_model.h5')
print("Model saved")

valid_generator = valid_datagen.flow_from_directory(
    directory=validation_data_dir,
    target_size=(224, 224),
    color_mode="rgb",
    batch_size=1,
    class_mode="categorical",
    shuffle=False,
    seed=42
)

(eval_loss, eval_accuracy) = model.evaluate_generator(generator=valid_generator,
steps=STEP_SIZE_VALID)
print((eval_loss, eval_accuracy))

STEP_SIZE_TEST=test_generator.n//test_generator.batch_size
test_generator.reset()
pred=model.predict_generator(test_generator,
steps=STEP_SIZE_TEST,
verbose=1)
print(STEP_SIZE_TEST)

import numpy as np
predicted_class_indices=np.argmax(pred,axis=1)

labels = (train_generator.class_indices)
labels = dict((v,k) for k,v in labels.items())
predictions = [labels[k] for k in predicted_class_indices]

import pandas as pd
filenames = []
# filenames=test_generator.filenames
for filename in test_generator.filenames:
  filename = filename.split("/")[-1]
  filenames.append(filename)
print(test_generator.filenames)
results=pd.DataFrame({"Image":filenames,
                      "target":predictions})

results.to_csv("results1.csv",index=False)

